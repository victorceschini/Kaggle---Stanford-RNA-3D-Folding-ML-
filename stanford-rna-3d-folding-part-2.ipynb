{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79bf72a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:26:56.557570Z",
     "iopub.status.busy": "2026-02-16T21:26:56.557248Z",
     "iopub.status.idle": "2026-02-16T21:27:10.885149Z",
     "shell.execute_reply": "2026-02-16T21:27:10.884296Z"
    },
    "papermill": {
     "duration": 14.335319,
     "end_time": "2026-02-16T21:27:10.887245",
     "exception": false,
     "start_time": "2026-02-16T21:26:56.551926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Processing /kaggle/input/datasets/kami1976/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython==1.86) (2.0.2)\r\n",
      "Installing collected packages: biopython\r\n",
      "Successfully installed biopython-1.86\r\n"
     ]
    }
   ],
   "source": [
    "# ——— Environment and imports ———\n",
    "\n",
    "!pip install xgboost scikit-learn\n",
    "!pip install /kaggle/input/datasets/kami1976/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os, sys, time\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Bio imports\n",
    "from Bio.Align import PairwiseAligner\n",
    "\n",
    "# ML imports\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7746c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:10.895857Z",
     "iopub.status.busy": "2026-02-16T21:27:10.895346Z",
     "iopub.status.idle": "2026-02-16T21:27:21.730174Z",
     "shell.execute_reply": "2026-02-16T21:27:21.729275Z"
    },
    "papermill": {
     "duration": 10.841425,
     "end_time": "2026-02-16T21:27:21.732214",
     "exception": false,
     "start_time": "2026-02-16T21:27:10.890789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ——— Load CSVs from competition input ———\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/kaggle/input/competitions/stanford-rna-3d-folding-2'\n",
    "train_seqs = pd.read_csv(DATA_PATH + '/train_sequences.csv')\n",
    "test_seqs = pd.read_csv(DATA_PATH + '/test_sequences.csv')\n",
    "train_labels = pd.read_csv(DATA_PATH + '/train_labels.csv')\n",
    "\n",
    "sys.path.append(os.path.join(DATA_PATH, \"/extra\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863f10d7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:21.740443Z",
     "iopub.status.busy": "2026-02-16T21:27:21.740106Z",
     "iopub.status.idle": "2026-02-16T21:27:22.384411Z",
     "shell.execute_reply": "2026-02-16T21:27:22.383221Z"
    },
    "papermill": {
     "duration": 0.650894,
     "end_time": "2026-02-16T21:27:22.386520",
     "exception": false,
     "start_time": "2026-02-16T21:27:21.735626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Robust import for Kaggle's extra/parse_fasta_py.py ---\n",
    "try:\n",
    "    import typing as _typing\n",
    "    import builtins as _builtins\n",
    "\n",
    "    # Make these names available during module import-time annotation evaluation\n",
    "    _builtins.Dict  = getattr(_typing, \"Dict\")\n",
    "    _builtins.Tuple = getattr(_typing, \"Tuple\")\n",
    "    _builtins.List  = getattr(_typing, \"List\")\n",
    "\n",
    "    from parse_fasta_py import parse_fasta as _parse_fasta_raw\n",
    "\n",
    "    # Normalize output to: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        d = _parse_fasta_raw(fasta_content)\n",
    "        out = {}\n",
    "        for k, v in d.items():\n",
    "            # some variants return (sequence, headers/lines) or similar\n",
    "            out[k] = v[0] if isinstance(v, tuple) else v\n",
    "        return out\n",
    "\n",
    "except Exception:\n",
    "    # Fallback FASTA parser: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        out = {}\n",
    "        cur = None\n",
    "        seq_parts = []\n",
    "        for line in str(fasta_content).splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if cur is not None:\n",
    "                    out[cur] = \"\".join(seq_parts)\n",
    "                header = line[1:]\n",
    "                # First token is usually chain id in this dataset\n",
    "                cur = header.split()[0]\n",
    "                seq_parts = []\n",
    "            else:\n",
    "                seq_parts.append(line.replace(\" \", \"\"))\n",
    "        if cur is not None:\n",
    "            out[cur] = \"\".join(seq_parts)\n",
    "        return out\n",
    "\n",
    "def parse_stoichiometry(stoich: str):\n",
    "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
    "        return []\n",
    "    out = []\n",
    "    for part in str(stoich).split(';'):\n",
    "        ch, cnt = part.split(':')\n",
    "        out.append((ch.strip(), int(cnt)))\n",
    "    return out\n",
    "\n",
    "def get_chain_segments(row):\n",
    "    \"\"\"\n",
    "    Returns list of (start,end) segments in row['sequence'] corresponding to chain copies in stoichiometry order.\n",
    "    Falls back to single segment if parsing fails.\n",
    "    \"\"\"\n",
    "    seq = row['sequence']\n",
    "    stoich = row.get('stoichiometry', '')\n",
    "    all_seq = row.get('all_sequences', '')\n",
    "\n",
    "    if pd.isna(stoich) or pd.isna(all_seq) or str(stoich).strip()==\"\" or str(all_seq).strip()==\"\":\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "    try:\n",
    "        chain_dict = parse_fasta(all_seq)  # dict: chain_id -> sequence\n",
    "        order = parse_stoichiometry(stoich)\n",
    "        segs = []\n",
    "        pos = 0\n",
    "        for ch, cnt in order:\n",
    "            base = chain_dict.get(ch)\n",
    "            if base is None:\n",
    "                return [(0, len(seq))]\n",
    "            for _ in range(cnt):\n",
    "                L = len(base)\n",
    "                segs.append((pos, pos + L))\n",
    "                pos += L\n",
    "        if pos != len(seq):\n",
    "            return [(0, len(seq))]\n",
    "        return segs\n",
    "    except Exception:\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "# Build maps for train and test for quick use\n",
    "def build_segments_map(df):\n",
    "    seg_map = {}\n",
    "    stoich_map = {}\n",
    "    for _, r in df.iterrows():\n",
    "        tid = r['target_id']\n",
    "        seg_map[tid] = get_chain_segments(r)\n",
    "        stoich_map[tid] = str(r.get('stoichiometry', '') if not pd.isna(r.get('stoichiometry', '')) else '')\n",
    "    return seg_map, stoich_map\n",
    "\n",
    "train_segs_map, train_stoich_map = build_segments_map(train_seqs)\n",
    "test_segs_map,  test_stoich_map  = build_segments_map(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e7043f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:22.394779Z",
     "iopub.status.busy": "2026-02-16T21:27:22.394415Z",
     "iopub.status.idle": "2026-02-16T21:27:37.265503Z",
     "shell.execute_reply": "2026-02-16T21:27:37.264615Z"
    },
    "papermill": {
     "duration": 14.877871,
     "end_time": "2026-02-16T21:27:37.267730",
     "exception": false,
     "start_time": "2026-02-16T21:27:22.389859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ——— Process train_labels into dict of coords per target_id ———\n",
    "\n",
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    # Faster + safer prefix extraction\n",
    "    prefixes = labels_df['ID'].str.rsplit('_', n=1).str[0]\n",
    "    for id_prefix, group in labels_df.groupby(prefixes):\n",
    "        coords_dict[id_prefix] = group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values\n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7aa1665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:37.275888Z",
     "iopub.status.busy": "2026-02-16T21:27:37.275500Z",
     "iopub.status.idle": "2026-02-16T21:27:37.281990Z",
     "shell.execute_reply": "2026-02-16T21:27:37.281125Z"
    },
    "papermill": {
     "duration": 0.012657,
     "end_time": "2026-02-16T21:27:37.283670",
     "exception": false,
     "start_time": "2026-02-16T21:27:37.271013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ——— Aligner: Biopython ———\n",
    "\n",
    "# Scoring parameters\n",
    "MATCH_SCORE = 2.0\n",
    "MISMATCH_SCORE = -1.6\n",
    "# Residue numbering must match\n",
    "GAP_OPEN = -8.0\n",
    "GAP_EXTEND = -0.4\n",
    "\n",
    "# Create Aligner\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = 'global'\n",
    "\n",
    "# Define scores\n",
    "aligner.match_score = MATCH_SCORE\n",
    "aligner.mismatch_score = MISMATCH_SCORE\n",
    "aligner.open_gap_score = GAP_OPEN\n",
    "aligner.extend_gap_score = GAP_EXTEND\n",
    "\n",
    "# Penalize terminal gaps\n",
    "aligner.query_left_open_gap_score  = GAP_OPEN\n",
    "aligner.query_left_extend_gap_score = GAP_EXTEND\n",
    "aligner.query_right_open_gap_score = GAP_OPEN\n",
    "aligner.query_right_extend_gap_score = GAP_EXTEND\n",
    "aligner.target_left_open_gap_score = GAP_OPEN\n",
    "aligner.target_left_extend_gap_score = GAP_EXTEND\n",
    "aligner.target_right_open_gap_score = GAP_OPEN\n",
    "aligner.target_right_extend_gap_score = GAP_EXTEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615841df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:37.292035Z",
     "iopub.status.busy": "2026-02-16T21:27:37.291713Z",
     "iopub.status.idle": "2026-02-16T21:27:37.326817Z",
     "shell.execute_reply": "2026-02-16T21:27:37.325653Z"
    },
    "papermill": {
     "duration": 0.041806,
     "end_time": "2026-02-16T21:27:37.328753",
     "exception": false,
     "start_time": "2026-02-16T21:27:37.286947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    similar_seqs = []\n",
    "    \n",
    "    # Pre-filter: Iterate only valid targets\n",
    "    # Note: aligner.score is much faster than generating full alignments\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id, train_seq = row['target_id'], row['sequence']\n",
    "        if target_id not in train_coords_dict: continue\n",
    "        \n",
    "        # Length filter (keep your original logic)\n",
    "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.3: continue\n",
    "        \n",
    "        # FAST SCORE: Calculates score without traceback overhead\n",
    "        raw_score = aligner.score(query_seq, train_seq)\n",
    "        \n",
    "        normalized_score = raw_score / (2 * min(len(query_seq), len(train_seq)))\n",
    "        similar_seqs.append((target_id, train_seq, normalized_score, train_coords_dict[target_id]))\n",
    "    \n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_seqs[:top_n]\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords):\n",
    "    # Generate the alignment object\n",
    "    # aligner.align returns an iterator; we take the first optimal alignment\n",
    "    alignment = next(iter(aligner.align(query_seq, template_seq)))\n",
    "    \n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    \n",
    "    # VECTORIZED MAPPING:\n",
    "    # alignment.aligned returns lists of (start, end) tuples for matched segments.\n",
    "    # This avoids the slow python loop \"for char_q, char_t in zip...\"\n",
    "    for (q_start, q_end), (t_start, t_end) in zip(*alignment.aligned):\n",
    "        # Map the coordinate chunk directly\n",
    "        t_chunk = template_coords[t_start:t_end]\n",
    "        \n",
    "        # Safety check to ensure shapes match (handles edge cases)\n",
    "        if len(t_chunk) == (q_end - q_start):\n",
    "            new_coords[q_start:q_end] = t_chunk\n",
    "\n",
    "    # --- Interpolation Logic (Unchanged) ---\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if prev_v >= 0 and next_v >= 0:\n",
    "                w = (i - prev_v) / (next_v - prev_v)\n",
    "                new_coords[i] = (1-w)*new_coords[prev_v] + w*new_coords[next_v]\n",
    "            elif prev_v >= 0: new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
    "            elif next_v >= 0: new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
    "            else: new_coords[i] = [i*3, 0, 0]\n",
    "            \n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "def adaptive_rna_constraints(coordinates, target_id, confidence=1.0, passes=2):\n",
    "    \"\"\"\n",
    "    Evaluation-driven constraints:\n",
    "    - US-align is show-only rigid body => internal geometry errors are fatal\n",
    "    - apply within each chain segment (no fake bond across chain breaks)\n",
    "    \"\"\"\n",
    "    coords = coordinates.copy()\n",
    "    segments = test_segs_map.get(target_id, [(0, len(coords))])\n",
    "\n",
    "    # stronger corrections when confidence is low\n",
    "    strength = 0.80 * (1.0 - min(confidence, 0.98))\n",
    "    strength = max(strength, 0.02)\n",
    "\n",
    "    for _ in range(passes):\n",
    "        for (s, e) in segments:\n",
    "            X = coords[s:e]\n",
    "            L = e - s\n",
    "            if L < 3:\n",
    "                coords[s:e] = X\n",
    "                continue\n",
    "\n",
    "            # (1) bond i,i+1 to ~5.95Å (vectorized, symmetric)\n",
    "            d = X[1:] - X[:-1]\n",
    "            dist = np.linalg.norm(d, axis=1) + 1e-5\n",
    "            target = 5.95\n",
    "            scale = (target - dist) / dist\n",
    "            adj = (d * scale[:, None]) * (0.22 * strength)\n",
    "            X[:-1] -= adj\n",
    "            X[1:]  += adj\n",
    "\n",
    "            # (2) soft i,i+2 to ~10.2Å (vectorized, symmetric)\n",
    "            d2 = X[2:] - X[:-2]\n",
    "            dist2 = np.linalg.norm(d2, axis=1) + 1e-6\n",
    "            target2 = 10.2\n",
    "            scale2 = (target2 - dist2) / dist2\n",
    "            adj2 = (d2 * scale2[:, None]) * (0.10 * strength)\n",
    "            X[:-2] -= adj2\n",
    "            X[2:]  += adj2\n",
    "\n",
    "            # (3) Laplacian smoothing (removes kinks US-align cannot fix)\n",
    "            lap = 0.5 * (X[:-2] + X[2:]) - X[1:-1]\n",
    "            X[1:-1] += (0.06 * strength) * lap\n",
    "\n",
    "            # (4) light self-avoidance (prevents steric collapse)\n",
    "            if L >= 25:\n",
    "                k = min(L, 160) if L > 220 else L\n",
    "                if k < L:\n",
    "                    idx = np.linspace(0, L - 1, k).astype(int)\n",
    "                else:\n",
    "                    idx = np.arange(L)\n",
    "\n",
    "                P = X[idx]\n",
    "                diff = P[:, None, :] - P[None, :, :]\n",
    "                distm = np.linalg.norm(diff, axis=2) + 1e-6\n",
    "                sep = np.abs(idx[:, None] - idx[None, :])\n",
    "\n",
    "                mask = (sep > 2) & (distm < 3.3)\n",
    "                if np.any(mask):\n",
    "                    force = (3.3 - distm) / distm\n",
    "                    vec = (diff * force[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
    "                    X[idx] += (0.015 * strength) * vec\n",
    "\n",
    "            coords[s:e] = X\n",
    "\n",
    "    return coords\n",
    "\n",
    "def _rotmat(axis, ang):\n",
    "    axis = np.asarray(axis, float)\n",
    "    axis = axis / (np.linalg.norm(axis) + 1e-12)\n",
    "    x, y, z = axis\n",
    "    c, s = np.cos(ang), np.sin(ang)\n",
    "    C = 1.0 - c\n",
    "    return np.array([\n",
    "        [c + x*x*C,     x*y*C - z*s, x*z*C + y*s],\n",
    "        [y*x*C + z*s,   c + y*y*C,   y*z*C - x*s],\n",
    "        [z*x*C - y*s,   z*y*C + x*s, c + z*z*C]\n",
    "    ], dtype=float)\n",
    "\n",
    "def apply_hinge(coords, seg, rng, max_angle_deg=25):\n",
    "    s, e = seg\n",
    "    L = e - s\n",
    "    if L < 30:\n",
    "        return coords\n",
    "    pivot = s + int(rng.integers(10, L - 10))\n",
    "    axis = rng.normal(size=3)\n",
    "    ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "    R = _rotmat(axis, ang)\n",
    "    X = coords.copy()\n",
    "    p0 = X[pivot].copy()\n",
    "    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n",
    "    return X\n",
    "\n",
    "def jitter_chains(coords, segments, rng, max_angle_deg=12, max_trans=1.5):\n",
    "    X = coords.copy()\n",
    "    global_center = X.mean(axis=0, keepdims=True)\n",
    "    for (s, e) in segments:\n",
    "        axis = rng.normal(size=3)\n",
    "        ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "        R = _rotmat(axis, ang)\n",
    "        shift = rng.normal(size=3)\n",
    "        shift = shift / (np.linalg.norm(shift) + 1e-10) * float(rng.uniform(0.0, max_trans))\n",
    "        c = X[s:e].mean(axis=0, keepdims=True)\n",
    "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
    "    # recenter\n",
    "    X -= X.mean(axis=0, keepdims=True) - global_center\n",
    "    return X\n",
    "\n",
    "def smooth_wiggle(coords, segments, rng, amp=0.8):\n",
    "    X = coords.copy()\n",
    "    for (s, e) in segments:\n",
    "        L = e - s\n",
    "        if L < 20:\n",
    "            continue\n",
    "        n_ctrl = 6\n",
    "        ctrl_x = np.linspace(0, L - 1, n_ctrl)\n",
    "        ctrl_disp = rng.normal(0, amp, size=(n_ctrl, 3))\n",
    "        t = np.arange(L)\n",
    "        disp = np.vstack([np.interp(t, ctrl_x, ctrl_disp[:, k]) for k in range(3)]).T\n",
    "        X[s:e] += disp\n",
    "    return X\n",
    "\n",
    "def predict_rna_structures(row, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    tid = row['target_id']\n",
    "    seq = row['sequence']\n",
    "\n",
    "    # Data constraint: should already be canonical A/C/G/U\n",
    "    assert set(seq).issubset(set(\"ACGU\")), f\"Non-ACGU in {tid}; do not remap here.\"\n",
    "\n",
    "    segments = test_segs_map.get(tid, [(0, len(seq))])\n",
    "\n",
    "    # Grab a larger candidate pool, then sample for diversity (best-of-5)\n",
    "    cands = find_similar_sequences(query_seq=seq, train_seqs_df=train_seqs_df, train_coords_dict=train_coords_dict, top_n=40)\n",
    "    assert all(len(c[3]) == len(c[1]) for c in cands), \"Template coords/seq length mismatch\"\n",
    "    predictions = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        seed = (abs(hash(tid)) + i * 10005) % (2**32)\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        if not cands:\n",
    "            # hard fallback (straight line per chain)\n",
    "            coords = np.zeros((len(seq), 3), dtype=float)\n",
    "            for (s, e) in segments:\n",
    "                for j in range(s+1, e):\n",
    "                    coords[j] = coords[j-1] + [5.95, 0, 0]\n",
    "            predictions.append(coords)\n",
    "            continue\n",
    "\n",
    "        # Choose template:\n",
    "        # i=0 => best template; others => sample among top-K with weights, avoid duplicates\n",
    "        if i == 0:\n",
    "            t_id, t_seq, sim, t_coords = cands[0]\n",
    "        else:\n",
    "            K = min(12, len(cands))\n",
    "            sims = np.array([cands[k][2] for k in range(K)], float)\n",
    "            w = np.exp((sims - sims.max()) / 0.10)\n",
    "            # penalize already used templates\n",
    "            for k in range(K):\n",
    "                if cands[k][0] in used:\n",
    "                    w[k] *= 0.10\n",
    "            w = w / (w.sum() + 1e-10)\n",
    "            k = int(rng.choice(np.arange(K), p=w))\n",
    "            t_id, t_seq, sim, t_coords = cands[k]\n",
    "\n",
    "        used.add(t_id)\n",
    "\n",
    "        # Transfer coords with diagonal-guard mapping (no sliding)\n",
    "        adapted = adapt_template_to_query(query_seq=seq, template_seq=t_seq, template_coords=t_coords)\n",
    "\n",
    "        # Diversity transforms (then re-refine constraints)\n",
    "        if i == 0:\n",
    "            X = adapted\n",
    "        elif i == 1:\n",
    "            # mild noise\n",
    "            X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
    "        elif i == 2:\n",
    "            # hinge within the longest chain\n",
    "            longest = max(segments, key=lambda se: se[1] - se[0])\n",
    "            X = apply_hinge(adapted, longest, rng, max_angle_deg=22)\n",
    "        elif i == 3:\n",
    "            # inter-chain jitter (small, safe)\n",
    "            X = jitter_chains(adapted, segments, rng, max_angle_deg=10, max_trans=1.0)\n",
    "        else:\n",
    "            # smooth low-frequency deformation\n",
    "            X = smooth_wiggle(adapted, segments, rng, amp=0.8)\n",
    "\n",
    "        refined = adaptive_rna_constraints(X, tid, confidence=sim, passes=2)\n",
    "        predictions.append(refined)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4841aae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:37.336308Z",
     "iopub.status.busy": "2026-02-16T21:27:37.335992Z",
     "iopub.status.idle": "2026-02-16T21:27:37.343233Z",
     "shell.execute_reply": "2026-02-16T21:27:37.342412Z"
    },
    "papermill": {
     "duration": 0.013198,
     "end_time": "2026-02-16T21:27:37.345045",
     "exception": false,
     "start_time": "2026-02-16T21:27:37.331847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Provide window one-hot encoding and k-mer helpers used for prefiltering and features ---\n",
    "\n",
    "NT_ORDER = ['A','C','G','U']\n",
    "nt_to_onehot = {nt: np.array([1 if nt == x else 0 for x in NT_ORDER]) for nt in NT_ORDER}\n",
    "\n",
    "def seq_window_onehot(seq, idx, W=2):\n",
    "    vecs = []\n",
    "    L = len(seq)\n",
    "    for k in range(idx - W, idx + W + 1):\n",
    "        if 0 <= k < L:\n",
    "            v = nt_to_onehot.get(seq[k], np.zeros(4))\n",
    "        else:\n",
    "            v = np.zeros(4)\n",
    "        vecs.append(v)\n",
    "    return np.concatenate(vecs)  # (2W+1)*4\n",
    "\n",
    "def kmer_set(s, k=3):\n",
    "    if len(s) < k:\n",
    "        return set([s])\n",
    "    return set(s[i:i+k] for i in range(len(s)-k+1))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    ia = a & b\n",
    "    return len(ia) / len(a | b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd36fd59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:37.353824Z",
     "iopub.status.busy": "2026-02-16T21:27:37.353472Z",
     "iopub.status.idle": "2026-02-16T21:27:37.368728Z",
     "shell.execute_reply": "2026-02-16T21:27:37.367758Z"
    },
    "papermill": {
     "duration": 0.022484,
     "end_time": "2026-02-16T21:27:37.370712",
     "exception": false,
     "start_time": "2026-02-16T21:27:37.348228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- build residual dataset ---\n",
    "\n",
    "def build_residual_dataset(train_seqs_df, train_coords_dict, \n",
    "                           top_template_k=5, window=2, max_targets=None,\n",
    "                           max_pos_per_target=None, sample_positions=False,\n",
    "                           kmer_prefilter_top=None):\n",
    "    \"\"\"\n",
    "    Builds features and targets (residuals) to train models.\n",
    "    Parameters:\n",
    "      - max_targets: limit on the number of targets to speed up testing.\n",
    "      - max_pos_per_target: limit on sampled positions per target (useful for testing).\n",
    "      - sample_positions: if True, randomly samples positions; if False, uses all up to the limit.\n",
    "      - kmer_prefilter_top: if int, pre-filters templates by top-N Jaccard k-mer before using the aligner (speeds up).\n",
    "    Returns (feats_df, targets_array).\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    targets = []\n",
    "    nt_window_size = window\n",
    "\n",
    "    # precompute kmer sets\n",
    "    kmer_sets = {}\n",
    "    if kmer_prefilter_top:\n",
    "        for _, r in train_seqs_df.iterrows():\n",
    "            kmer_sets[r['target_id']] = kmer_set(r['sequence'], k=3)\n",
    "\n",
    "    ids = list(train_seqs_df['target_id'])\n",
    "    if max_targets:\n",
    "        ids = ids[:max_targets]\n",
    "\n",
    "    for tid in ids:\n",
    "        row = train_seqs_df[train_seqs_df['target_id'] == tid].iloc[0]\n",
    "        seq = row['sequence']\n",
    "        L = len(seq)\n",
    "        if tid not in train_coords_dict:\n",
    "            continue\n",
    "        true_coords = train_coords_dict[tid]  # shape (L,3) expected\n",
    "\n",
    "        # skip if true coords have NaN everywhere or shape mismatch\n",
    "        if true_coords is None or true_coords.shape[0] != L:\n",
    "            continue\n",
    "\n",
    "        # choose template candidates\n",
    "        if kmer_prefilter_top:\n",
    "            q_k = kmer_set(seq, k=3)\n",
    "            # compute jaccard to all targets quickly\n",
    "            scores = []\n",
    "            for _, r2 in train_seqs_df.iterrows():\n",
    "                tid2 = r2['target_id']\n",
    "                if tid2 not in train_coords_dict or tid2 == tid:\n",
    "                    continue\n",
    "                j = jaccard(q_k, kmer_sets[tid2])\n",
    "                scores.append((tid2, j))\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            cand_ids = [s[0] for s in scores[: max(200, top_template_k*10) ]]  # top candidates\n",
    "            # build a filtered DataFrame of candidates\n",
    "            cand_df = train_seqs_df[train_seqs_df['target_id'].isin(cand_ids)]\n",
    "            cands = find_similar_sequences(seq, cand_df, train_coords_dict, top_n=top_template_k)\n",
    "        else:\n",
    "            cands = find_similar_sequences(seq, train_seqs_df, train_coords_dict, top_n=top_template_k)\n",
    "\n",
    "        if not cands:\n",
    "            continue\n",
    "\n",
    "        t_id, t_seq, sim, t_coords = cands[0]\n",
    "        adapted = adapt_template_to_query(seq, t_seq, t_coords)\n",
    "\n",
    "        # build list of positions to include\n",
    "        pos_indices = list(range(L))\n",
    "        if max_pos_per_target:\n",
    "            if sample_positions:\n",
    "                rng = np.random.default_rng(abs(hash(tid)) % (2**32))\n",
    "                pos_indices = rng.choice(np.arange(L), size=min(max_pos_per_target, L), replace=False).tolist()\n",
    "            else:\n",
    "                pos_indices = list(range(min(L, max_pos_per_target)))\n",
    "\n",
    "        # iterate positions\n",
    "        for i in pos_indices:\n",
    "            true_pt = true_coords[i]\n",
    "            adapt_pt = adapted[i]\n",
    "            # filter if any NaN or infinite\n",
    "            if not np.isfinite(true_pt).all():\n",
    "                continue\n",
    "            if not np.isfinite(adapt_pt).all():\n",
    "                continue\n",
    "            # compute features\n",
    "            feat = {}\n",
    "            feat['target_id'] = tid\n",
    "            feat['pos'] = i\n",
    "            feat['pos_norm'] = i / max(1, L-1)\n",
    "            feat['seq_len'] = L\n",
    "            feat['sim_score'] = sim\n",
    "            feat['ax'], feat['ay'], feat['az'] = adapt_pt\n",
    "            # window one-hot\n",
    "            win = seq_window_onehot(seq, i, W=nt_window_size)\n",
    "            for j, val in enumerate(win):\n",
    "                feat[f'w_{j}'] = float(val)\n",
    "            rows.append(feat)\n",
    "            # target residual\n",
    "            rx, ry, rz = true_pt - adapt_pt\n",
    "            targets.append((float(rx), float(ry), float(rz)))\n",
    "\n",
    "    if not rows:\n",
    "        # nothing built\n",
    "        return pd.DataFrame(), np.zeros((0,3), dtype=float)\n",
    "\n",
    "    feats_df = pd.DataFrame(rows)\n",
    "    targets_arr = np.array(targets, dtype=float)\n",
    "    # Ensure no rows with NaN remain\n",
    "    finite_mask = np.isfinite(targets_arr).all(axis=1)\n",
    "    feats_df = feats_df.loc[finite_mask].reset_index(drop=True)\n",
    "    targets_arr = targets_arr[finite_mask]\n",
    "\n",
    "    return feats_df, targets_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69de93d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:27:37.378704Z",
     "iopub.status.busy": "2026-02-16T21:27:37.378366Z",
     "iopub.status.idle": "2026-02-16T21:28:37.269852Z",
     "shell.execute_reply": "2026-02-16T21:28:37.268592Z"
    },
    "papermill": {
     "duration": 59.901298,
     "end_time": "2026-02-16T21:28:37.275268",
     "exception": false,
     "start_time": "2026-02-16T21:27:37.373970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7230\n"
     ]
    }
   ],
   "source": [
    "# --- Quick build for development and inspect coverage ---\n",
    "\n",
    "feats, targets = build_residual_dataset(train_seqs, train_coords_dict,\n",
    "                                       top_template_k=5, window=2,\n",
    "                                       max_targets=200, max_pos_per_target=100,\n",
    "                                       sample_positions=True, kmer_prefilter_top=200)\n",
    "\n",
    "if feats.shape[0] == 0:\n",
    "    raise RuntimeError(\"No samples built - check train_coords_dict and functions adapt_template_to_query/find_similar_sequences.\")\n",
    "\n",
    "# define feature columns (all except identifiers)\n",
    "feature_cols = [c for c in feats.columns if c not in ['target_id','pos']]\n",
    "\n",
    "X = feats[feature_cols].values\n",
    "y = targets  # (N,3)\n",
    "groups = feats['target_id'].values\n",
    "\n",
    "# clean NaNs / infinites just for safety\n",
    "if y.ndim == 1:\n",
    "    mask_y = np.isfinite(y)\n",
    "else:\n",
    "    mask_y = np.isfinite(y).all(axis=1)\n",
    "mask_good = np.isfinite(X).all(axis=1) & mask_y\n",
    "\n",
    "X = X[mask_good]\n",
    "y = y[mask_good]\n",
    "groups = groups[mask_good]\n",
    "\n",
    "print(\"Training samples:\", X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5389fbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:28:37.283905Z",
     "iopub.status.busy": "2026-02-16T21:28:37.283442Z",
     "iopub.status.idle": "2026-02-16T21:29:00.320097Z",
     "shell.execute_reply": "2026-02-16T21:29:00.318874Z"
    },
    "papermill": {
     "duration": 23.043497,
     "end_time": "2026-02-16T21:29:00.322146",
     "exception": false,
     "start_time": "2026-02-16T21:28:37.278649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: (7230, 26) Targets: (7230, 3)\n",
      "Unique train targets: 196\n",
      "Fold 1/5 -> RF RMSE: 25.1005 | XGB RMSE: 20.8089 | ENS RMSE: 21.7658\n",
      "Fold 2/5 -> RF RMSE: 29.0151 | XGB RMSE: 30.3002 | ENS RMSE: 28.9025\n",
      "Fold 3/5 -> RF RMSE: 58.3969 | XGB RMSE: 56.3454 | ENS RMSE: 55.8658\n",
      "Fold 4/5 -> RF RMSE: 38.1238 | XGB RMSE: 59.5341 | ENS RMSE: 46.9830\n",
      "Fold 5/5 -> RF RMSE: 30.9320 | XGB RMSE: 37.4067 | ENS RMSE: 32.1382\n",
      "CV mean RMSE -> RF: 36.31364560229808 XGB: 40.87905199766221 ENS: 37.1310764759451\n",
      "Training final models on the entire dataset (may take a while)...\n",
      "Models and artifacts saved: rf_final.joblib, xgb_final.joblib, feature_cols.joblib, model_metadata.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- Training + Cross-validation ---\n",
    "\n",
    "print(\"Samples:\", X.shape, \"Targets:\", y.shape)\n",
    "unique_targets = len(np.unique(groups))\n",
    "print(\"Unique train targets:\", unique_targets)\n",
    "\n",
    "# Hyperparameters\n",
    "rf_params = dict(n_estimators=120, max_depth=18, n_jobs=-1, random_state=42)\n",
    "xgb_base = xgb.XGBRegressor(n_estimators=200, max_depth=6, tree_method='hist', verbosity=0, random_state=42)\n",
    "\n",
    "# Cross-validation by target\n",
    "n_splits = min(5, max(2, unique_targets))\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "rf_fold_scores = []\n",
    "xgb_fold_scores = []\n",
    "ens_fold_scores = []\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in gkf.split(X, y, groups):\n",
    "    fold += 1\n",
    "    Xtr, Xval = X[train_idx], X[val_idx]\n",
    "    ytr, yval = y[train_idx], y[val_idx]\n",
    "\n",
    "    # RF multi-output\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    rf.fit(Xtr, ytr)\n",
    "\n",
    "    # XGB multi-output (wrapper)\n",
    "    xgb_multi = MultiOutputRegressor(xgb_base, n_jobs=3)\n",
    "    xgb_multi.fit(Xtr, ytr)\n",
    "\n",
    "    # Prediction and evaluation\n",
    "    p_rf = rf.predict(Xval)\n",
    "    p_xg = xgb_multi.predict(Xval)\n",
    "    p_ens = (p_rf + p_xg) / 2.0\n",
    "\n",
    "    rmse_rf = math.sqrt(mean_squared_error(yval, p_rf))\n",
    "    rmse_xg = math.sqrt(mean_squared_error(yval, p_xg))\n",
    "    rmse_ens = math.sqrt(mean_squared_error(yval, p_ens))\n",
    "\n",
    "    print(f\"Fold {fold}/{n_splits} -> RF RMSE: {rmse_rf:.4f} | XGB RMSE: {rmse_xg:.4f} | ENS RMSE: {rmse_ens:.4f}\")\n",
    "\n",
    "    rf_fold_scores.append(rmse_rf)\n",
    "    xgb_fold_scores.append(rmse_xg)\n",
    "    ens_fold_scores.append(rmse_ens)\n",
    "\n",
    "print(\"CV mean RMSE -> RF:\", np.mean(rf_fold_scores), \"XGB:\", np.mean(xgb_fold_scores), \"ENS:\", np.mean(ens_fold_scores))\n",
    "\n",
    "# Final training on the whole dataset\n",
    "print(\"Training final models on the entire dataset (may take a while)...\")\n",
    "rf_final = RandomForestRegressor(**rf_params)\n",
    "rf_final.fit(X, y)\n",
    "\n",
    "xgb_final = MultiOutputRegressor(xgb_base, n_jobs=3)\n",
    "xgb_final.fit(X, y)\n",
    "\n",
    "# Save models and metadata\n",
    "joblib.dump(rf_final, 'rf_final.joblib')\n",
    "joblib.dump(xgb_final, 'xgb_final.joblib')\n",
    "\n",
    "# Save feature_cols and metadata (important for inference)\n",
    "feature_cols = sorted(feature_cols)\n",
    "joblib.dump(feature_cols, 'feature_cols.joblib')\n",
    "metadata = {'window': 2, 'feature_cols_len': len(feature_cols)}  # adjust 'window' if using another\n",
    "joblib.dump(metadata, 'model_metadata.joblib')\n",
    "\n",
    "print(\"Models and artifacts saved: rf_final.joblib, xgb_final.joblib, feature_cols.joblib, model_metadata.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe7da64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:29:00.331621Z",
     "iopub.status.busy": "2026-02-16T21:29:00.331055Z",
     "iopub.status.idle": "2026-02-16T21:31:35.104808Z",
     "shell.execute_reply": "2026-02-16T21:31:35.103776Z"
    },
    "papermill": {
     "duration": 154.780702,
     "end_time": "2026-02-16T21:31:35.106649",
     "exception": false,
     "start_time": "2026-02-16T21:29:00.325947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 | 0.0s\n",
      "Processing 10 | 135.1s\n",
      "Processing 20 | 142.6s\n",
      "submission.csv saved with 9762 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ZNQ_1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>161.694678</td>\n",
       "      <td>182.648592</td>\n",
       "      <td>196.881677</td>\n",
       "      <td>-31.575007</td>\n",
       "      <td>-24.985690</td>\n",
       "      <td>16.865704</td>\n",
       "      <td>6.977852</td>\n",
       "      <td>-40.847913</td>\n",
       "      <td>12.180672</td>\n",
       "      <td>5.124279</td>\n",
       "      <td>-18.471605</td>\n",
       "      <td>41.666795</td>\n",
       "      <td>64.896253</td>\n",
       "      <td>-3.506433</td>\n",
       "      <td>1.282732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8ZNQ_2</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>157.666965</td>\n",
       "      <td>174.819268</td>\n",
       "      <td>196.019805</td>\n",
       "      <td>-28.665769</td>\n",
       "      <td>-26.994636</td>\n",
       "      <td>21.476258</td>\n",
       "      <td>10.807732</td>\n",
       "      <td>-40.002283</td>\n",
       "      <td>15.922828</td>\n",
       "      <td>11.465611</td>\n",
       "      <td>-16.192393</td>\n",
       "      <td>39.534814</td>\n",
       "      <td>64.726635</td>\n",
       "      <td>-8.409687</td>\n",
       "      <td>6.136341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ZNQ_3</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>153.218130</td>\n",
       "      <td>175.983745</td>\n",
       "      <td>199.085249</td>\n",
       "      <td>-26.986273</td>\n",
       "      <td>-28.245894</td>\n",
       "      <td>26.678762</td>\n",
       "      <td>6.884595</td>\n",
       "      <td>-32.006772</td>\n",
       "      <td>12.771456</td>\n",
       "      <td>8.195892</td>\n",
       "      <td>-11.239635</td>\n",
       "      <td>31.342928</td>\n",
       "      <td>57.874354</td>\n",
       "      <td>-10.870275</td>\n",
       "      <td>6.836482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ZNQ_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>157.710612</td>\n",
       "      <td>178.736468</td>\n",
       "      <td>205.385907</td>\n",
       "      <td>-22.315158</td>\n",
       "      <td>-27.631069</td>\n",
       "      <td>29.425970</td>\n",
       "      <td>10.966890</td>\n",
       "      <td>-27.334953</td>\n",
       "      <td>14.741784</td>\n",
       "      <td>10.855853</td>\n",
       "      <td>-12.775670</td>\n",
       "      <td>29.973608</td>\n",
       "      <td>57.786435</td>\n",
       "      <td>-15.780308</td>\n",
       "      <td>9.059042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8ZNQ_5</td>\n",
       "      <td>U</td>\n",
       "      <td>5</td>\n",
       "      <td>158.227952</td>\n",
       "      <td>182.084280</td>\n",
       "      <td>206.835447</td>\n",
       "      <td>-17.525756</td>\n",
       "      <td>-26.580142</td>\n",
       "      <td>21.277124</td>\n",
       "      <td>13.920715</td>\n",
       "      <td>-23.974731</td>\n",
       "      <td>10.068780</td>\n",
       "      <td>13.055820</td>\n",
       "      <td>-15.332476</td>\n",
       "      <td>25.611703</td>\n",
       "      <td>54.226545</td>\n",
       "      <td>-18.795122</td>\n",
       "      <td>6.216053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8ZNQ_6</td>\n",
       "      <td>G</td>\n",
       "      <td>6</td>\n",
       "      <td>156.810619</td>\n",
       "      <td>185.981081</td>\n",
       "      <td>209.079932</td>\n",
       "      <td>-13.285221</td>\n",
       "      <td>-25.442847</td>\n",
       "      <td>21.657890</td>\n",
       "      <td>14.471840</td>\n",
       "      <td>-21.845770</td>\n",
       "      <td>4.935187</td>\n",
       "      <td>16.752743</td>\n",
       "      <td>-17.721862</td>\n",
       "      <td>21.286940</td>\n",
       "      <td>54.517795</td>\n",
       "      <td>-22.738465</td>\n",
       "      <td>3.176257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8ZNQ_7</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>156.981249</td>\n",
       "      <td>190.634683</td>\n",
       "      <td>210.641121</td>\n",
       "      <td>-10.184631</td>\n",
       "      <td>-21.791500</td>\n",
       "      <td>19.668594</td>\n",
       "      <td>9.774395</td>\n",
       "      <td>-38.512517</td>\n",
       "      <td>5.534942</td>\n",
       "      <td>12.466790</td>\n",
       "      <td>-20.530829</td>\n",
       "      <td>20.322148</td>\n",
       "      <td>55.477534</td>\n",
       "      <td>-27.274411</td>\n",
       "      <td>7.541017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8ZNQ_8</td>\n",
       "      <td>C</td>\n",
       "      <td>8</td>\n",
       "      <td>153.121369</td>\n",
       "      <td>189.607596</td>\n",
       "      <td>210.502018</td>\n",
       "      <td>-7.754230</td>\n",
       "      <td>-18.768145</td>\n",
       "      <td>16.454905</td>\n",
       "      <td>1.910345</td>\n",
       "      <td>-38.809355</td>\n",
       "      <td>1.175913</td>\n",
       "      <td>7.379107</td>\n",
       "      <td>-22.825547</td>\n",
       "      <td>16.304402</td>\n",
       "      <td>61.432370</td>\n",
       "      <td>-29.321875</td>\n",
       "      <td>7.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8ZNQ_9</td>\n",
       "      <td>G</td>\n",
       "      <td>9</td>\n",
       "      <td>150.449939</td>\n",
       "      <td>191.055532</td>\n",
       "      <td>209.232872</td>\n",
       "      <td>-12.651575</td>\n",
       "      <td>-13.885579</td>\n",
       "      <td>18.683422</td>\n",
       "      <td>-4.250051</td>\n",
       "      <td>-45.252148</td>\n",
       "      <td>3.053867</td>\n",
       "      <td>5.356645</td>\n",
       "      <td>-28.412325</td>\n",
       "      <td>20.434603</td>\n",
       "      <td>59.600489</td>\n",
       "      <td>-26.452647</td>\n",
       "      <td>5.104496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8ZNQ_10</td>\n",
       "      <td>G</td>\n",
       "      <td>10</td>\n",
       "      <td>150.303806</td>\n",
       "      <td>190.821752</td>\n",
       "      <td>207.859128</td>\n",
       "      <td>-16.782862</td>\n",
       "      <td>-8.699739</td>\n",
       "      <td>18.408282</td>\n",
       "      <td>-9.755507</td>\n",
       "      <td>-39.593742</td>\n",
       "      <td>1.286989</td>\n",
       "      <td>0.551812</td>\n",
       "      <td>-24.360974</td>\n",
       "      <td>15.848723</td>\n",
       "      <td>62.361756</td>\n",
       "      <td>-26.294710</td>\n",
       "      <td>3.420751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID resname  resid         x_1         y_1         z_1        x_2  \\\n",
       "0   8ZNQ_1       A      1  161.694678  182.648592  196.881677 -31.575007   \n",
       "1   8ZNQ_2       C      2  157.666965  174.819268  196.019805 -28.665769   \n",
       "2   8ZNQ_3       C      3  153.218130  175.983745  199.085249 -26.986273   \n",
       "3   8ZNQ_4       G      4  157.710612  178.736468  205.385907 -22.315158   \n",
       "4   8ZNQ_5       U      5  158.227952  182.084280  206.835447 -17.525756   \n",
       "5   8ZNQ_6       G      6  156.810619  185.981081  209.079932 -13.285221   \n",
       "6   8ZNQ_7       A      7  156.981249  190.634683  210.641121 -10.184631   \n",
       "7   8ZNQ_8       C      8  153.121369  189.607596  210.502018  -7.754230   \n",
       "8   8ZNQ_9       G      9  150.449939  191.055532  209.232872 -12.651575   \n",
       "9  8ZNQ_10       G     10  150.303806  190.821752  207.859128 -16.782862   \n",
       "\n",
       "         y_2        z_2        x_3        y_3        z_3        x_4  \\\n",
       "0 -24.985690  16.865704   6.977852 -40.847913  12.180672   5.124279   \n",
       "1 -26.994636  21.476258  10.807732 -40.002283  15.922828  11.465611   \n",
       "2 -28.245894  26.678762   6.884595 -32.006772  12.771456   8.195892   \n",
       "3 -27.631069  29.425970  10.966890 -27.334953  14.741784  10.855853   \n",
       "4 -26.580142  21.277124  13.920715 -23.974731  10.068780  13.055820   \n",
       "5 -25.442847  21.657890  14.471840 -21.845770   4.935187  16.752743   \n",
       "6 -21.791500  19.668594   9.774395 -38.512517   5.534942  12.466790   \n",
       "7 -18.768145  16.454905   1.910345 -38.809355   1.175913   7.379107   \n",
       "8 -13.885579  18.683422  -4.250051 -45.252148   3.053867   5.356645   \n",
       "9  -8.699739  18.408282  -9.755507 -39.593742   1.286989   0.551812   \n",
       "\n",
       "         y_4        z_4        x_5        y_5       z_5  \n",
       "0 -18.471605  41.666795  64.896253  -3.506433  1.282732  \n",
       "1 -16.192393  39.534814  64.726635  -8.409687  6.136341  \n",
       "2 -11.239635  31.342928  57.874354 -10.870275  6.836482  \n",
       "3 -12.775670  29.973608  57.786435 -15.780308  9.059042  \n",
       "4 -15.332476  25.611703  54.226545 -18.795122  6.216053  \n",
       "5 -17.721862  21.286940  54.517795 -22.738465  3.176257  \n",
       "6 -20.530829  20.322148  55.477534 -27.274411  7.541017  \n",
       "7 -22.825547  16.304402  61.432370 -29.321875  7.020902  \n",
       "8 -28.412325  20.434603  59.600489 -26.452647  5.104496  \n",
       "9 -24.360974  15.848723  62.361756 -26.294710  3.420751  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Inference: robust prediction + aggregation into submission.csv ---\n",
    "\n",
    "# load models in case it's a new session\n",
    "feature_cols = joblib.load('feature_cols.joblib')\n",
    "metadata = joblib.load('model_metadata.joblib')\n",
    "rf_model = joblib.load('rf_final.joblib')\n",
    "xgb_model = joblib.load('xgb_final.joblib')\n",
    "\n",
    "# Models per axis\n",
    "rf_per_ch = None\n",
    "xgb_per_ch = None\n",
    "if rf_model is None:\n",
    "    rf_ch_files = sorted([f for f in os.listdir('.') if f.startswith('rf_ch') and f.endswith('.joblib')])\n",
    "    if rf_ch_files:\n",
    "        rf_per_ch = [joblib.load(f) for f in rf_ch_files]\n",
    "        print(\"RF per axis loaded:\", rf_ch_files)\n",
    "if xgb_model is None:\n",
    "    xgb_ch_files = sorted([f for f in os.listdir('.') if f.startswith('xgb_ch') and f.endswith('.joblib')])\n",
    "    if xgb_ch_files:\n",
    "        xgb_per_ch = [joblib.load(f) for f in xgb_ch_files]\n",
    "        print(\"XGB per axis loaded:\", xgb_ch_files)\n",
    "\n",
    "if rf_model is None and rf_per_ch is None:\n",
    "    raise FileNotFoundError(\"No RF model found (rf_final.joblib or rf_ch*.joblib).\")\n",
    "\n",
    "# Residuals prediction function\n",
    "def predict_residuals_for_adapted(feat_df):\n",
    "    for c in feature_cols:\n",
    "        if c not in feat_df.columns:\n",
    "            feat_df[c] = 0.0\n",
    "    feat_df = feat_df[feature_cols].astype(float)\n",
    "    Xmat = feat_df.values\n",
    "\n",
    "    # RF predict\n",
    "    if rf_model is not None:\n",
    "        p_rf = rf_model.predict(Xmat)  # (N,3)\n",
    "    else:\n",
    "        preds = [m.predict(Xmat) for m in rf_per_ch]\n",
    "        p_rf = np.column_stack(preds)\n",
    "\n",
    "    # XGB predict\n",
    "    if xgb_model is not None:\n",
    "        p_xg = xgb_model.predict(Xmat)\n",
    "    elif xgb_per_ch is not None:\n",
    "        preds = [m.predict(Xmat) for m in xgb_per_ch]\n",
    "        p_xg = np.column_stack(preds)\n",
    "    else:\n",
    "        p_xg = None\n",
    "\n",
    "    if p_xg is None:\n",
    "        return p_rf\n",
    "    return (p_rf + p_xg) / 2.0  # simple average\n",
    "\n",
    "# iterate test set and aggregate by ID\n",
    "pred_map = {}  # key: ID -> dict with all columns x_1..x_5,y_1..y_5,z_1..z_5, resname, resid\n",
    "missing_counts_before = 0\n",
    "total_rows = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Processing {idx} | {time.time()-start_time:.1f}s\")\n",
    "    tid, seq = row['target_id'], row['sequence']\n",
    "    preds_adapted = predict_rna_structures(row, train_seqs, train_coords_dict, n_predictions=5)\n",
    "    # if fewer than 5 returned, fill logically with what is available\n",
    "    n_preds = len(preds_adapted)\n",
    "    if n_preds == 0:\n",
    "        # fallback: straight line per chain\n",
    "        segments = test_segs_map.get(tid, [(0, len(seq))])\n",
    "        coords = np.zeros((len(seq),3), dtype=float)\n",
    "        for (s,e) in segments:\n",
    "            for j in range(s+1,e):\n",
    "                coords[j] = coords[j-1] + [5.95,0,0]\n",
    "        preds_adapted = [coords]  # one candidate\n",
    "\n",
    "    for i_pred, adapted_coords in enumerate(preds_adapted):\n",
    "        # apply residuals prediction for all residues in this adapted_coords\n",
    "        L = len(seq)\n",
    "        feat_rows = []\n",
    "        W = metadata.get('window', 2)\n",
    "        for i in range(L):\n",
    "            f = {}\n",
    "            f['pos'] = i\n",
    "            f['pos_norm'] = i / max(1, L-1)\n",
    "            f['seq_len'] = L\n",
    "            f['sim_score'] = 0.0\n",
    "            f['ax'], f['ay'], f['az'] = adapted_coords[i]\n",
    "            w = seq_window_onehot(seq, i, W=W)\n",
    "            for j, val in enumerate(w):\n",
    "                f[f'w_{j}'] = float(val)\n",
    "            feat_rows.append(f)\n",
    "        feat_df = pd.DataFrame(feat_rows)\n",
    "        # ensure column order\n",
    "        for c in feature_cols:\n",
    "            if c not in feat_df.columns:\n",
    "                feat_df[c] = 0.0\n",
    "        feat_df = feat_df[feature_cols]\n",
    "\n",
    "        resid_pred = predict_residuals_for_adapted(feat_df)  # (L,3)\n",
    "        corrected = adapted_coords + resid_pred\n",
    "        refined = adaptive_rna_constraints(corrected, tid, confidence=0.8, passes=2)\n",
    "\n",
    "        # fill map by ID\n",
    "        for j in range(L):\n",
    "            total_rows += 1\n",
    "            ID = f\"{tid}_{j+1}\"\n",
    "            # create base entry if not exists\n",
    "            if ID not in pred_map:\n",
    "                # initialize all columns x_1..x_5,y_1..y_5,z_1..z_5 with NaN (will fill later)\n",
    "                d = {'ID': ID, 'resname': seq[j], 'resid': j+1}\n",
    "                for k in range(1,6):\n",
    "                    d[f'x_{k}'] = np.nan\n",
    "                    d[f'y_{k}'] = np.nan\n",
    "                    d[f'z_{k}'] = np.nan\n",
    "                pred_map[ID] = d\n",
    "            # set corresponding triplet (i_pred may be >=5 if function generates more; limit to 5)\n",
    "            col_idx = i_pred + 1\n",
    "            if col_idx <= 5:\n",
    "                pred_map[ID][f'x_{col_idx}'] = float(refined[j,0])\n",
    "                pred_map[ID][f'y_{col_idx}'] = float(refined[j,1])\n",
    "                pred_map[ID][f'z_{col_idx}'] = float(refined[j,2])\n",
    "\n",
    "# Convert to DataFrame and sort by ID/resid (optional: keep test_seqs order)\n",
    "rows = list(pred_map.values())\n",
    "sub = pd.DataFrame(rows)\n",
    "# ensure expected columns and order: ID,resname,resid,x_1,y_1,z_1,...,x_5,y_5,z_5\n",
    "cols = ['ID','resname','resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\n",
    "for c in cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = 0.0\n",
    "sub = sub[cols]\n",
    "\n",
    "# clip coords and save\n",
    "coord_cols = [c for c in cols if c.startswith(('x_','y_','z_'))]\n",
    "sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv saved with\", len(sub), \"rows.\")\n",
    "sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "isSourceIdPinned": false,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "datasetId": 9328538,
     "sourceId": 14604295,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 284.656912,
   "end_time": "2026-02-16T21:31:37.831817",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T21:26:53.174905",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
