{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 118765,
          "databundleVersionId": 15231210,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 14604295,
          "sourceType": "datasetVersion",
          "datasetId": 9328538
        }
      ],
      "dockerImageVersionId": 31259,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Stanford RNA 3D Folding Part 2",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "sE9xxcBlyMK-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "stanford_rna_3d_folding_2_path = kagglehub.competition_download('stanford-rna-3d-folding-2')\n",
        "kami1976_biopython_cp312_path = kagglehub.dataset_download('kami1976/biopython-cp312')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ndJSbLpryMK_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— Environment and imports ———\n",
        "\n",
        "!pip install xgboost scikit-learn\n",
        "!pip install /kaggle/input/datasets/kami1976/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os, sys, time\n",
        "import warnings\n",
        "import math\n",
        "\n",
        "# Bio imports\n",
        "from Bio.Align import PairwiseAligner\n",
        "\n",
        "# ML imports\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:22.66514Z",
          "iopub.execute_input": "2026-02-16T21:17:22.66551Z",
          "iopub.status.idle": "2026-02-16T21:17:30.9875Z",
          "shell.execute_reply.started": "2026-02-16T21:17:22.665482Z",
          "shell.execute_reply": "2026-02-16T21:17:30.98641Z"
        },
        "id": "kD2-n7nAyMLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— Load CSVs from competition input ———\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_PATH = '/kaggle/input/competitions/stanford-rna-3d-folding-2'\n",
        "train_seqs = pd.read_csv(DATA_PATH + '/train_sequences.csv')\n",
        "test_seqs = pd.read_csv(DATA_PATH + '/test_sequences.csv')\n",
        "train_labels = pd.read_csv(DATA_PATH + '/train_labels.csv')\n",
        "\n",
        "sys.path.append(os.path.join(DATA_PATH, \"/extra\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:30.990322Z",
          "iopub.execute_input": "2026-02-16T21:17:30.991085Z",
          "iopub.status.idle": "2026-02-16T21:17:40.218762Z",
          "shell.execute_reply.started": "2026-02-16T21:17:30.991047Z",
          "shell.execute_reply": "2026-02-16T21:17:40.21779Z"
        },
        "id": "M5LpzvLXyMLB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Robust import for Kaggle's extra/parse_fasta_py.py ---\n",
        "try:\n",
        "    import typing as _typing\n",
        "    import builtins as _builtins\n",
        "\n",
        "    # Make these names available during module import-time annotation evaluation\n",
        "    _builtins.Dict  = getattr(_typing, \"Dict\")\n",
        "    _builtins.Tuple = getattr(_typing, \"Tuple\")\n",
        "    _builtins.List  = getattr(_typing, \"List\")\n",
        "\n",
        "    from parse_fasta_py import parse_fasta as _parse_fasta_raw\n",
        "\n",
        "    # Normalize output to: {chain_id: sequence_string}\n",
        "    def parse_fasta(fasta_content: str):\n",
        "        d = _parse_fasta_raw(fasta_content)\n",
        "        out = {}\n",
        "        for k, v in d.items():\n",
        "            # some variants return (sequence, headers/lines) or similar\n",
        "            out[k] = v[0] if isinstance(v, tuple) else v\n",
        "        return out\n",
        "\n",
        "except Exception:\n",
        "    # Fallback FASTA parser: {chain_id: sequence_string}\n",
        "    def parse_fasta(fasta_content: str):\n",
        "        out = {}\n",
        "        cur = None\n",
        "        seq_parts = []\n",
        "        for line in str(fasta_content).splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if line.startswith(\">\"):\n",
        "                if cur is not None:\n",
        "                    out[cur] = \"\".join(seq_parts)\n",
        "                header = line[1:]\n",
        "                # First token is usually chain id in this dataset\n",
        "                cur = header.split()[0]\n",
        "                seq_parts = []\n",
        "            else:\n",
        "                seq_parts.append(line.replace(\" \", \"\"))\n",
        "        if cur is not None:\n",
        "            out[cur] = \"\".join(seq_parts)\n",
        "        return out\n",
        "\n",
        "def parse_stoichiometry(stoich: str):\n",
        "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
        "        return []\n",
        "    out = []\n",
        "    for part in str(stoich).split(';'):\n",
        "        ch, cnt = part.split(':')\n",
        "        out.append((ch.strip(), int(cnt)))\n",
        "    return out\n",
        "\n",
        "def get_chain_segments(row):\n",
        "    \"\"\"\n",
        "    Returns list of (start,end) segments in row['sequence'] corresponding to chain copies in stoichiometry order.\n",
        "    Falls back to single segment if parsing fails.\n",
        "    \"\"\"\n",
        "    seq = row['sequence']\n",
        "    stoich = row.get('stoichiometry', '')\n",
        "    all_seq = row.get('all_sequences', '')\n",
        "\n",
        "    if pd.isna(stoich) or pd.isna(all_seq) or str(stoich).strip()==\"\" or str(all_seq).strip()==\"\":\n",
        "        return [(0, len(seq))]\n",
        "\n",
        "    try:\n",
        "        chain_dict = parse_fasta(all_seq)  # dict: chain_id -> sequence\n",
        "        order = parse_stoichiometry(stoich)\n",
        "        segs = []\n",
        "        pos = 0\n",
        "        for ch, cnt in order:\n",
        "            base = chain_dict.get(ch)\n",
        "            if base is None:\n",
        "                return [(0, len(seq))]\n",
        "            for _ in range(cnt):\n",
        "                L = len(base)\n",
        "                segs.append((pos, pos + L))\n",
        "                pos += L\n",
        "        if pos != len(seq):\n",
        "            return [(0, len(seq))]\n",
        "        return segs\n",
        "    except Exception:\n",
        "        return [(0, len(seq))]\n",
        "\n",
        "# Build maps for train and test for quick use\n",
        "def build_segments_map(df):\n",
        "    seg_map = {}\n",
        "    stoich_map = {}\n",
        "    for _, r in df.iterrows():\n",
        "        tid = r['target_id']\n",
        "        seg_map[tid] = get_chain_segments(r)\n",
        "        stoich_map[tid] = str(r.get('stoichiometry', '') if not pd.isna(r.get('stoichiometry', '')) else '')\n",
        "    return seg_map, stoich_map\n",
        "\n",
        "train_segs_map, train_stoich_map = build_segments_map(train_seqs)\n",
        "test_segs_map,  test_stoich_map  = build_segments_map(test_seqs)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:40.219941Z",
          "iopub.execute_input": "2026-02-16T21:17:40.220221Z",
          "iopub.status.idle": "2026-02-16T21:17:40.828845Z",
          "shell.execute_reply.started": "2026-02-16T21:17:40.220187Z",
          "shell.execute_reply": "2026-02-16T21:17:40.827894Z"
        },
        "id": "Pgkv3NfWyMLB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— Process train_labels into dict of coords per target_id ———\n",
        "\n",
        "def process_labels(labels_df):\n",
        "    coords_dict = {}\n",
        "    # Faster + safer prefix extraction\n",
        "    prefixes = labels_df['ID'].str.rsplit('_', n=1).str[0]\n",
        "    for id_prefix, group in labels_df.groupby(prefixes):\n",
        "        coords_dict[id_prefix] = group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values\n",
        "    return coords_dict\n",
        "\n",
        "train_coords_dict = process_labels(train_labels)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:40.82988Z",
          "iopub.execute_input": "2026-02-16T21:17:40.83014Z",
          "iopub.status.idle": "2026-02-16T21:17:55.772794Z",
          "shell.execute_reply.started": "2026-02-16T21:17:40.830116Z",
          "shell.execute_reply": "2026-02-16T21:17:55.772021Z"
        },
        "id": "dY2xVNamyMLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— Aligner: Biopython ———\n",
        "\n",
        "# Scoring parameters\n",
        "MATCH_SCORE = 2.0\n",
        "MISMATCH_SCORE = -1.6\n",
        "# Residue numbering must match\n",
        "GAP_OPEN = -8.0\n",
        "GAP_EXTEND = -0.4\n",
        "\n",
        "# Create Aligner\n",
        "aligner = PairwiseAligner()\n",
        "aligner.mode = 'global'\n",
        "\n",
        "# Define scores\n",
        "aligner.match_score = MATCH_SCORE\n",
        "aligner.mismatch_score = MISMATCH_SCORE\n",
        "aligner.open_gap_score = GAP_OPEN\n",
        "aligner.extend_gap_score = GAP_EXTEND\n",
        "\n",
        "# Penalize terminal gaps\n",
        "aligner.query_left_open_gap_score  = GAP_OPEN\n",
        "aligner.query_left_extend_gap_score = GAP_EXTEND\n",
        "aligner.query_right_open_gap_score = GAP_OPEN\n",
        "aligner.query_right_extend_gap_score = GAP_EXTEND\n",
        "aligner.target_left_open_gap_score = GAP_OPEN\n",
        "aligner.target_left_extend_gap_score = GAP_EXTEND\n",
        "aligner.target_right_open_gap_score = GAP_OPEN\n",
        "aligner.target_right_extend_gap_score = GAP_EXTEND"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:55.775547Z",
          "iopub.execute_input": "2026-02-16T21:17:55.775849Z",
          "iopub.status.idle": "2026-02-16T21:17:55.782522Z",
          "shell.execute_reply.started": "2026-02-16T21:17:55.775823Z",
          "shell.execute_reply": "2026-02-16T21:17:55.781691Z"
        },
        "id": "xKrMHTF4yMLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
        "    similar_seqs = []\n",
        "\n",
        "    # Pre-filter: Iterate only valid targets\n",
        "    # Note: aligner.score is much faster than generating full alignments\n",
        "    for _, row in train_seqs_df.iterrows():\n",
        "        target_id, train_seq = row['target_id'], row['sequence']\n",
        "        if target_id not in train_coords_dict: continue\n",
        "\n",
        "        # Length filter (keep your original logic)\n",
        "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.3: continue\n",
        "\n",
        "        # FAST SCORE: Calculates score without traceback overhead\n",
        "        raw_score = aligner.score(query_seq, train_seq)\n",
        "\n",
        "        normalized_score = raw_score / (2 * min(len(query_seq), len(train_seq)))\n",
        "        similar_seqs.append((target_id, train_seq, normalized_score, train_coords_dict[target_id]))\n",
        "\n",
        "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
        "    return similar_seqs[:top_n]\n",
        "\n",
        "def adapt_template_to_query(query_seq, template_seq, template_coords):\n",
        "    # Generate the alignment object\n",
        "    # aligner.align returns an iterator; we take the first optimal alignment\n",
        "    alignment = next(iter(aligner.align(query_seq, template_seq)))\n",
        "\n",
        "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
        "\n",
        "    # VECTORIZED MAPPING:\n",
        "    # alignment.aligned returns lists of (start, end) tuples for matched segments.\n",
        "    # This avoids the slow python loop \"for char_q, char_t in zip...\"\n",
        "    for (q_start, q_end), (t_start, t_end) in zip(*alignment.aligned):\n",
        "        # Map the coordinate chunk directly\n",
        "        t_chunk = template_coords[t_start:t_end]\n",
        "\n",
        "        # Safety check to ensure shapes match (handles edge cases)\n",
        "        if len(t_chunk) == (q_end - q_start):\n",
        "            new_coords[q_start:q_end] = t_chunk\n",
        "\n",
        "    # --- Interpolation Logic (Unchanged) ---\n",
        "    for i in range(len(new_coords)):\n",
        "        if np.isnan(new_coords[i, 0]):\n",
        "            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
        "            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
        "            if prev_v >= 0 and next_v >= 0:\n",
        "                w = (i - prev_v) / (next_v - prev_v)\n",
        "                new_coords[i] = (1-w)*new_coords[prev_v] + w*new_coords[next_v]\n",
        "            elif prev_v >= 0: new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
        "            elif next_v >= 0: new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
        "            else: new_coords[i] = [i*3, 0, 0]\n",
        "\n",
        "    return np.nan_to_num(new_coords)\n",
        "\n",
        "def adaptive_rna_constraints(coordinates, target_id, confidence=1.0, passes=2):\n",
        "    \"\"\"\n",
        "    Evaluation-driven constraints:\n",
        "    - US-align is show-only rigid body => internal geometry errors are fatal\n",
        "    - apply within each chain segment (no fake bond across chain breaks)\n",
        "    \"\"\"\n",
        "    coords = coordinates.copy()\n",
        "    segments = test_segs_map.get(target_id, [(0, len(coords))])\n",
        "\n",
        "    # stronger corrections when confidence is low\n",
        "    strength = 0.80 * (1.0 - min(confidence, 0.98))\n",
        "    strength = max(strength, 0.02)\n",
        "\n",
        "    for _ in range(passes):\n",
        "        for (s, e) in segments:\n",
        "            X = coords[s:e]\n",
        "            L = e - s\n",
        "            if L < 3:\n",
        "                coords[s:e] = X\n",
        "                continue\n",
        "\n",
        "            # (1) bond i,i+1 to ~5.95Å (vectorized, symmetric)\n",
        "            d = X[1:] - X[:-1]\n",
        "            dist = np.linalg.norm(d, axis=1) + 1e-5\n",
        "            target = 5.95\n",
        "            scale = (target - dist) / dist\n",
        "            adj = (d * scale[:, None]) * (0.22 * strength)\n",
        "            X[:-1] -= adj\n",
        "            X[1:]  += adj\n",
        "\n",
        "            # (2) soft i,i+2 to ~10.2Å (vectorized, symmetric)\n",
        "            d2 = X[2:] - X[:-2]\n",
        "            dist2 = np.linalg.norm(d2, axis=1) + 1e-6\n",
        "            target2 = 10.2\n",
        "            scale2 = (target2 - dist2) / dist2\n",
        "            adj2 = (d2 * scale2[:, None]) * (0.10 * strength)\n",
        "            X[:-2] -= adj2\n",
        "            X[2:]  += adj2\n",
        "\n",
        "            # (3) Laplacian smoothing (removes kinks US-align cannot fix)\n",
        "            lap = 0.5 * (X[:-2] + X[2:]) - X[1:-1]\n",
        "            X[1:-1] += (0.06 * strength) * lap\n",
        "\n",
        "            # (4) light self-avoidance (prevents steric collapse)\n",
        "            if L >= 25:\n",
        "                k = min(L, 160) if L > 220 else L\n",
        "                if k < L:\n",
        "                    idx = np.linspace(0, L - 1, k).astype(int)\n",
        "                else:\n",
        "                    idx = np.arange(L)\n",
        "\n",
        "                P = X[idx]\n",
        "                diff = P[:, None, :] - P[None, :, :]\n",
        "                distm = np.linalg.norm(diff, axis=2) + 1e-6\n",
        "                sep = np.abs(idx[:, None] - idx[None, :])\n",
        "\n",
        "                mask = (sep > 2) & (distm < 3.3)\n",
        "                if np.any(mask):\n",
        "                    force = (3.3 - distm) / distm\n",
        "                    vec = (diff * force[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
        "                    X[idx] += (0.015 * strength) * vec\n",
        "\n",
        "            coords[s:e] = X\n",
        "\n",
        "    return coords\n",
        "\n",
        "def _rotmat(axis, ang):\n",
        "    axis = np.asarray(axis, float)\n",
        "    axis = axis / (np.linalg.norm(axis) + 1e-12)\n",
        "    x, y, z = axis\n",
        "    c, s = np.cos(ang), np.sin(ang)\n",
        "    C = 1.0 - c\n",
        "    return np.array([\n",
        "        [c + x*x*C,     x*y*C - z*s, x*z*C + y*s],\n",
        "        [y*x*C + z*s,   c + y*y*C,   y*z*C - x*s],\n",
        "        [z*x*C - y*s,   z*y*C + x*s, c + z*z*C]\n",
        "    ], dtype=float)\n",
        "\n",
        "def apply_hinge(coords, seg, rng, max_angle_deg=25):\n",
        "    s, e = seg\n",
        "    L = e - s\n",
        "    if L < 30:\n",
        "        return coords\n",
        "    pivot = s + int(rng.integers(10, L - 10))\n",
        "    axis = rng.normal(size=3)\n",
        "    ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
        "    R = _rotmat(axis, ang)\n",
        "    X = coords.copy()\n",
        "    p0 = X[pivot].copy()\n",
        "    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n",
        "    return X\n",
        "\n",
        "def jitter_chains(coords, segments, rng, max_angle_deg=12, max_trans=1.5):\n",
        "    X = coords.copy()\n",
        "    global_center = X.mean(axis=0, keepdims=True)\n",
        "    for (s, e) in segments:\n",
        "        axis = rng.normal(size=3)\n",
        "        ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
        "        R = _rotmat(axis, ang)\n",
        "        shift = rng.normal(size=3)\n",
        "        shift = shift / (np.linalg.norm(shift) + 1e-10) * float(rng.uniform(0.0, max_trans))\n",
        "        c = X[s:e].mean(axis=0, keepdims=True)\n",
        "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
        "    # recenter\n",
        "    X -= X.mean(axis=0, keepdims=True) - global_center\n",
        "    return X\n",
        "\n",
        "def smooth_wiggle(coords, segments, rng, amp=0.8):\n",
        "    X = coords.copy()\n",
        "    for (s, e) in segments:\n",
        "        L = e - s\n",
        "        if L < 20:\n",
        "            continue\n",
        "        n_ctrl = 6\n",
        "        ctrl_x = np.linspace(0, L - 1, n_ctrl)\n",
        "        ctrl_disp = rng.normal(0, amp, size=(n_ctrl, 3))\n",
        "        t = np.arange(L)\n",
        "        disp = np.vstack([np.interp(t, ctrl_x, ctrl_disp[:, k]) for k in range(3)]).T\n",
        "        X[s:e] += disp\n",
        "    return X\n",
        "\n",
        "def predict_rna_structures(row, train_seqs_df, train_coords_dict, n_predictions=5):\n",
        "    tid = row['target_id']\n",
        "    seq = row['sequence']\n",
        "\n",
        "    # Data constraint: should already be canonical A/C/G/U\n",
        "    assert set(seq).issubset(set(\"ACGU\")), f\"Non-ACGU in {tid}; do not remap here.\"\n",
        "\n",
        "    segments = test_segs_map.get(tid, [(0, len(seq))])\n",
        "\n",
        "    # Grab a larger candidate pool, then sample for diversity (best-of-5)\n",
        "    cands = find_similar_sequences(query_seq=seq, train_seqs_df=train_seqs_df, train_coords_dict=train_coords_dict, top_n=40)\n",
        "    assert all(len(c[3]) == len(c[1]) for c in cands), \"Template coords/seq length mismatch\"\n",
        "    predictions = []\n",
        "    used = set()\n",
        "\n",
        "    for i in range(n_predictions):\n",
        "        seed = (abs(hash(tid)) + i * 10005) % (2**32)\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        if not cands:\n",
        "            # hard fallback (straight line per chain)\n",
        "            coords = np.zeros((len(seq), 3), dtype=float)\n",
        "            for (s, e) in segments:\n",
        "                for j in range(s+1, e):\n",
        "                    coords[j] = coords[j-1] + [5.95, 0, 0]\n",
        "            predictions.append(coords)\n",
        "            continue\n",
        "\n",
        "        # Choose template:\n",
        "        # i=0 => best template; others => sample among top-K with weights, avoid duplicates\n",
        "        if i == 0:\n",
        "            t_id, t_seq, sim, t_coords = cands[0]\n",
        "        else:\n",
        "            K = min(12, len(cands))\n",
        "            sims = np.array([cands[k][2] for k in range(K)], float)\n",
        "            w = np.exp((sims - sims.max()) / 0.10)\n",
        "            # penalize already used templates\n",
        "            for k in range(K):\n",
        "                if cands[k][0] in used:\n",
        "                    w[k] *= 0.10\n",
        "            w = w / (w.sum() + 1e-10)\n",
        "            k = int(rng.choice(np.arange(K), p=w))\n",
        "            t_id, t_seq, sim, t_coords = cands[k]\n",
        "\n",
        "        used.add(t_id)\n",
        "\n",
        "        # Transfer coords with diagonal-guard mapping (no sliding)\n",
        "        adapted = adapt_template_to_query(query_seq=seq, template_seq=t_seq, template_coords=t_coords)\n",
        "\n",
        "        # Diversity transforms (then re-refine constraints)\n",
        "        if i == 0:\n",
        "            X = adapted\n",
        "        elif i == 1:\n",
        "            # mild noise\n",
        "            X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
        "        elif i == 2:\n",
        "            # hinge within the longest chain\n",
        "            longest = max(segments, key=lambda se: se[1] - se[0])\n",
        "            X = apply_hinge(adapted, longest, rng, max_angle_deg=22)\n",
        "        elif i == 3:\n",
        "            # inter-chain jitter (small, safe)\n",
        "            X = jitter_chains(adapted, segments, rng, max_angle_deg=10, max_trans=1.0)\n",
        "        else:\n",
        "            # smooth low-frequency deformation\n",
        "            X = smooth_wiggle(adapted, segments, rng, amp=0.8)\n",
        "\n",
        "        refined = adaptive_rna_constraints(X, tid, confidence=sim, passes=2)\n",
        "        predictions.append(refined)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:55.783745Z",
          "iopub.execute_input": "2026-02-16T21:17:55.783993Z",
          "iopub.status.idle": "2026-02-16T21:17:55.827535Z",
          "shell.execute_reply.started": "2026-02-16T21:17:55.78397Z",
          "shell.execute_reply": "2026-02-16T21:17:55.826429Z"
        },
        "id": "DDfFclw5yMLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Provide window one-hot encoding and k-mer helpers used for prefiltering and features ---\n",
        "\n",
        "NT_ORDER = ['A','C','G','U']\n",
        "nt_to_onehot = {nt: np.array([1 if nt == x else 0 for x in NT_ORDER]) for nt in NT_ORDER}\n",
        "\n",
        "def seq_window_onehot(seq, idx, W=2):\n",
        "    vecs = []\n",
        "    L = len(seq)\n",
        "    for k in range(idx - W, idx + W + 1):\n",
        "        if 0 <= k < L:\n",
        "            v = nt_to_onehot.get(seq[k], np.zeros(4))\n",
        "        else:\n",
        "            v = np.zeros(4)\n",
        "        vecs.append(v)\n",
        "    return np.concatenate(vecs)  # (2W+1)*4\n",
        "\n",
        "def kmer_set(s, k=3):\n",
        "    if len(s) < k:\n",
        "        return set([s])\n",
        "    return set(s[i:i+k] for i in range(len(s)-k+1))\n",
        "\n",
        "def jaccard(a, b):\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    ia = a & b\n",
        "    return len(ia) / len(a | b)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:55.828592Z",
          "iopub.execute_input": "2026-02-16T21:17:55.828837Z",
          "iopub.status.idle": "2026-02-16T21:17:55.845605Z",
          "shell.execute_reply.started": "2026-02-16T21:17:55.828816Z",
          "shell.execute_reply": "2026-02-16T21:17:55.844759Z"
        },
        "id": "xu6M7_hwyMLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- build residual dataset ---\n",
        "\n",
        "def build_residual_dataset(train_seqs_df, train_coords_dict,\n",
        "                           top_template_k=5, window=2, max_targets=None,\n",
        "                           max_pos_per_target=None, sample_positions=False,\n",
        "                           kmer_prefilter_top=None):\n",
        "    \"\"\"\n",
        "    Builds features and targets (residuals) to train models.\n",
        "    Parameters:\n",
        "      - max_targets: limit on the number of targets to speed up testing.\n",
        "      - max_pos_per_target: limit on sampled positions per target (useful for testing).\n",
        "      - sample_positions: if True, randomly samples positions; if False, uses all up to the limit.\n",
        "      - kmer_prefilter_top: if int, pre-filters templates by top-N Jaccard k-mer before using the aligner (speeds up).\n",
        "    Returns (feats_df, targets_array).\n",
        "    \"\"\"\n",
        "\n",
        "    rows = []\n",
        "    targets = []\n",
        "    nt_window_size = window\n",
        "\n",
        "    # precompute kmer sets\n",
        "    kmer_sets = {}\n",
        "    if kmer_prefilter_top:\n",
        "        for _, r in train_seqs_df.iterrows():\n",
        "            kmer_sets[r['target_id']] = kmer_set(r['sequence'], k=3)\n",
        "\n",
        "    ids = list(train_seqs_df['target_id'])\n",
        "    if max_targets:\n",
        "        ids = ids[:max_targets]\n",
        "\n",
        "    for tid in ids:\n",
        "        row = train_seqs_df[train_seqs_df['target_id'] == tid].iloc[0]\n",
        "        seq = row['sequence']\n",
        "        L = len(seq)\n",
        "        if tid not in train_coords_dict:\n",
        "            continue\n",
        "        true_coords = train_coords_dict[tid]  # shape (L,3) expected\n",
        "\n",
        "        # skip if true coords have NaN everywhere or shape mismatch\n",
        "        if true_coords is None or true_coords.shape[0] != L:\n",
        "            continue\n",
        "\n",
        "        # choose template candidates\n",
        "        if kmer_prefilter_top:\n",
        "            q_k = kmer_set(seq, k=3)\n",
        "            # compute jaccard to all targets quickly\n",
        "            scores = []\n",
        "            for _, r2 in train_seqs_df.iterrows():\n",
        "                tid2 = r2['target_id']\n",
        "                if tid2 not in train_coords_dict or tid2 == tid:\n",
        "                    continue\n",
        "                j = jaccard(q_k, kmer_sets[tid2])\n",
        "                scores.append((tid2, j))\n",
        "            scores.sort(key=lambda x: x[1], reverse=True)\n",
        "            cand_ids = [s[0] for s in scores[: max(200, top_template_k*10) ]]  # top candidates\n",
        "            # build a filtered DataFrame of candidates\n",
        "            cand_df = train_seqs_df[train_seqs_df['target_id'].isin(cand_ids)]\n",
        "            cands = find_similar_sequences(seq, cand_df, train_coords_dict, top_n=top_template_k)\n",
        "        else:\n",
        "            cands = find_similar_sequences(seq, train_seqs_df, train_coords_dict, top_n=top_template_k)\n",
        "\n",
        "        if not cands:\n",
        "            continue\n",
        "\n",
        "        t_id, t_seq, sim, t_coords = cands[0]\n",
        "        adapted = adapt_template_to_query(seq, t_seq, t_coords)\n",
        "\n",
        "        # build list of positions to include\n",
        "        pos_indices = list(range(L))\n",
        "        if max_pos_per_target:\n",
        "            if sample_positions:\n",
        "                rng = np.random.default_rng(abs(hash(tid)) % (2**32))\n",
        "                pos_indices = rng.choice(np.arange(L), size=min(max_pos_per_target, L), replace=False).tolist()\n",
        "            else:\n",
        "                pos_indices = list(range(min(L, max_pos_per_target)))\n",
        "\n",
        "        # iterate positions\n",
        "        for i in pos_indices:\n",
        "            true_pt = true_coords[i]\n",
        "            adapt_pt = adapted[i]\n",
        "            # filter if any NaN or infinite\n",
        "            if not np.isfinite(true_pt).all():\n",
        "                continue\n",
        "            if not np.isfinite(adapt_pt).all():\n",
        "                continue\n",
        "            # compute features\n",
        "            feat = {}\n",
        "            feat['target_id'] = tid\n",
        "            feat['pos'] = i\n",
        "            feat['pos_norm'] = i / max(1, L-1)\n",
        "            feat['seq_len'] = L\n",
        "            feat['sim_score'] = sim\n",
        "            feat['ax'], feat['ay'], feat['az'] = adapt_pt\n",
        "            # window one-hot\n",
        "            win = seq_window_onehot(seq, i, W=nt_window_size)\n",
        "            for j, val in enumerate(win):\n",
        "                feat[f'w_{j}'] = float(val)\n",
        "            rows.append(feat)\n",
        "            # target residual\n",
        "            rx, ry, rz = true_pt - adapt_pt\n",
        "            targets.append((float(rx), float(ry), float(rz)))\n",
        "\n",
        "    if not rows:\n",
        "        # nothing built\n",
        "        return pd.DataFrame(), np.zeros((0,3), dtype=float)\n",
        "\n",
        "    feats_df = pd.DataFrame(rows)\n",
        "    targets_arr = np.array(targets, dtype=float)\n",
        "    # Ensure no rows with NaN remain\n",
        "    finite_mask = np.isfinite(targets_arr).all(axis=1)\n",
        "    feats_df = feats_df.loc[finite_mask].reset_index(drop=True)\n",
        "    targets_arr = targets_arr[finite_mask]\n",
        "\n",
        "    return feats_df, targets_arr"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:55.846653Z",
          "iopub.execute_input": "2026-02-16T21:17:55.846894Z",
          "iopub.status.idle": "2026-02-16T21:17:55.867267Z",
          "shell.execute_reply.started": "2026-02-16T21:17:55.846871Z",
          "shell.execute_reply": "2026-02-16T21:17:55.866547Z"
        },
        "id": "QE4a3JQ1yMLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Quick build for development and inspect coverage ---\n",
        "\n",
        "feats, targets = build_residual_dataset(train_seqs, train_coords_dict,\n",
        "                                       top_template_k=5, window=2,\n",
        "                                       max_targets=200, max_pos_per_target=100,\n",
        "                                       sample_positions=True, kmer_prefilter_top=200)\n",
        "\n",
        "if feats.shape[0] == 0:\n",
        "    raise RuntimeError(\"No samples built - check train_coords_dict and functions adapt_template_to_query/find_similar_sequences.\")\n",
        "\n",
        "# define feature columns (all except identifiers)\n",
        "feature_cols = [c for c in feats.columns if c not in ['target_id','pos']]\n",
        "\n",
        "X = feats[feature_cols].values\n",
        "y = targets  # (N,3)\n",
        "groups = feats['target_id'].values\n",
        "\n",
        "# clean NaNs / infinites just for safety\n",
        "if y.ndim == 1:\n",
        "    mask_y = np.isfinite(y)\n",
        "else:\n",
        "    mask_y = np.isfinite(y).all(axis=1)\n",
        "mask_good = np.isfinite(X).all(axis=1) & mask_y\n",
        "\n",
        "X = X[mask_good]\n",
        "y = y[mask_good]\n",
        "groups = groups[mask_good]\n",
        "\n",
        "print(\"Training samples:\", X.shape[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:17:55.868335Z",
          "iopub.execute_input": "2026-02-16T21:17:55.868637Z",
          "iopub.status.idle": "2026-02-16T21:19:06.19397Z",
          "shell.execute_reply.started": "2026-02-16T21:17:55.868605Z",
          "shell.execute_reply": "2026-02-16T21:19:06.193163Z"
        },
        "id": "T3i6dEmIyMLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training + Cross-validation ---\n",
        "\n",
        "print(\"Samples:\", X.shape, \"Targets:\", y.shape)\n",
        "unique_targets = len(np.unique(groups))\n",
        "print(\"Unique train targets:\", unique_targets)\n",
        "\n",
        "# Hyperparameters\n",
        "rf_params = dict(n_estimators=120, max_depth=18, n_jobs=-1, random_state=42)\n",
        "xgb_base = xgb.XGBRegressor(n_estimators=200, max_depth=6, tree_method='hist', verbosity=0, random_state=42)\n",
        "\n",
        "# Cross-validation by target\n",
        "n_splits = min(5, max(2, unique_targets))\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "rf_fold_scores = []\n",
        "xgb_fold_scores = []\n",
        "ens_fold_scores = []\n",
        "\n",
        "fold = 0\n",
        "for train_idx, val_idx in gkf.split(X, y, groups):\n",
        "    fold += 1\n",
        "    Xtr, Xval = X[train_idx], X[val_idx]\n",
        "    ytr, yval = y[train_idx], y[val_idx]\n",
        "\n",
        "    # RF multi-output\n",
        "    rf = RandomForestRegressor(**rf_params)\n",
        "    rf.fit(Xtr, ytr)\n",
        "\n",
        "    # XGB multi-output (wrapper)\n",
        "    xgb_multi = MultiOutputRegressor(xgb_base, n_jobs=3)\n",
        "    xgb_multi.fit(Xtr, ytr)\n",
        "\n",
        "    # Prediction and evaluation\n",
        "    p_rf = rf.predict(Xval)\n",
        "    p_xg = xgb_multi.predict(Xval)\n",
        "    p_ens = (p_rf + p_xg) / 2.0\n",
        "\n",
        "    rmse_rf = math.sqrt(mean_squared_error(yval, p_rf))\n",
        "    rmse_xg = math.sqrt(mean_squared_error(yval, p_xg))\n",
        "    rmse_ens = math.sqrt(mean_squared_error(yval, p_ens))\n",
        "\n",
        "    print(f\"Fold {fold}/{n_splits} -> RF RMSE: {rmse_rf:.4f} | XGB RMSE: {rmse_xg:.4f} | ENS RMSE: {rmse_ens:.4f}\")\n",
        "\n",
        "    rf_fold_scores.append(rmse_rf)\n",
        "    xgb_fold_scores.append(rmse_xg)\n",
        "    ens_fold_scores.append(rmse_ens)\n",
        "\n",
        "print(\"CV mean RMSE -> RF:\", np.mean(rf_fold_scores), \"XGB:\", np.mean(xgb_fold_scores), \"ENS:\", np.mean(ens_fold_scores))\n",
        "\n",
        "# Final training on the whole dataset\n",
        "print(\"Training final models on the entire dataset (may take a while)...\")\n",
        "rf_final = RandomForestRegressor(**rf_params)\n",
        "rf_final.fit(X, y)\n",
        "\n",
        "xgb_final = MultiOutputRegressor(xgb_base, n_jobs=3)\n",
        "xgb_final.fit(X, y)\n",
        "\n",
        "# Save models and metadata\n",
        "joblib.dump(rf_final, 'rf_final.joblib')\n",
        "joblib.dump(xgb_final, 'xgb_final.joblib')\n",
        "\n",
        "# Save feature_cols and metadata (important for inference)\n",
        "feature_cols = sorted(feature_cols)\n",
        "joblib.dump(feature_cols, 'feature_cols.joblib')\n",
        "metadata = {'window': 2, 'feature_cols_len': len(feature_cols)}  # adjust 'window' if using another\n",
        "joblib.dump(metadata, 'model_metadata.joblib')\n",
        "\n",
        "print(\"Models and artifacts saved: rf_final.joblib, xgb_final.joblib, feature_cols.joblib, model_metadata.joblib\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:19:06.195046Z",
          "iopub.execute_input": "2026-02-16T21:19:06.195732Z",
          "iopub.status.idle": "2026-02-16T21:19:33.409981Z",
          "shell.execute_reply.started": "2026-02-16T21:19:06.195697Z",
          "shell.execute_reply": "2026-02-16T21:19:33.409088Z"
        },
        "id": "EtNCuUs7yMLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Inference: robust prediction + aggregation into submission.csv ---\n",
        "\n",
        "# load models in case it's a new session\n",
        "feature_cols = joblib.load('feature_cols.joblib')\n",
        "metadata = joblib.load('model_metadata.joblib')\n",
        "rf_model = joblib.load('rf_final.joblib')\n",
        "xgb_model = joblib.load('xgb_final.joblib')\n",
        "\n",
        "# Models per axis\n",
        "rf_per_ch = None\n",
        "xgb_per_ch = None\n",
        "if rf_model is None:\n",
        "    rf_ch_files = sorted([f for f in os.listdir('.') if f.startswith('rf_ch') and f.endswith('.joblib')])\n",
        "    if rf_ch_files:\n",
        "        rf_per_ch = [joblib.load(f) for f in rf_ch_files]\n",
        "        print(\"RF per axis loaded:\", rf_ch_files)\n",
        "if xgb_model is None:\n",
        "    xgb_ch_files = sorted([f for f in os.listdir('.') if f.startswith('xgb_ch') and f.endswith('.joblib')])\n",
        "    if xgb_ch_files:\n",
        "        xgb_per_ch = [joblib.load(f) for f in xgb_ch_files]\n",
        "        print(\"XGB per axis loaded:\", xgb_ch_files)\n",
        "\n",
        "if rf_model is None and rf_per_ch is None:\n",
        "    raise FileNotFoundError(\"No RF model found (rf_final.joblib or rf_ch*.joblib).\")\n",
        "\n",
        "# Residuals prediction function\n",
        "def predict_residuals_for_adapted(feat_df):\n",
        "    for c in feature_cols:\n",
        "        if c not in feat_df.columns:\n",
        "            feat_df[c] = 0.0\n",
        "    feat_df = feat_df[feature_cols].astype(float)\n",
        "    Xmat = feat_df.values\n",
        "\n",
        "    # RF predict\n",
        "    if rf_model is not None:\n",
        "        p_rf = rf_model.predict(Xmat)  # (N,3)\n",
        "    else:\n",
        "        preds = [m.predict(Xmat) for m in rf_per_ch]\n",
        "        p_rf = np.column_stack(preds)\n",
        "\n",
        "    # XGB predict\n",
        "    if xgb_model is not None:\n",
        "        p_xg = xgb_model.predict(Xmat)\n",
        "    elif xgb_per_ch is not None:\n",
        "        preds = [m.predict(Xmat) for m in xgb_per_ch]\n",
        "        p_xg = np.column_stack(preds)\n",
        "    else:\n",
        "        p_xg = None\n",
        "\n",
        "    if p_xg is None:\n",
        "        return p_rf\n",
        "    return (p_rf + p_xg) / 2.0  # simple average\n",
        "\n",
        "# iterate test set and aggregate by ID\n",
        "pred_map = {}  # key: ID -> dict with all columns x_1..x_5,y_1..y_5,z_1..z_5, resname, resid\n",
        "missing_counts_before = 0\n",
        "total_rows = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, row in test_seqs.iterrows():\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Processing {idx} | {time.time()-start_time:.1f}s\")\n",
        "    tid, seq = row['target_id'], row['sequence']\n",
        "    preds_adapted = predict_rna_structures(row, train_seqs, train_coords_dict, n_predictions=5)\n",
        "    # if fewer than 5 returned, fill logically with what is available\n",
        "    n_preds = len(preds_adapted)\n",
        "    if n_preds == 0:\n",
        "        # fallback: straight line per chain\n",
        "        segments = test_segs_map.get(tid, [(0, len(seq))])\n",
        "        coords = np.zeros((len(seq),3), dtype=float)\n",
        "        for (s,e) in segments:\n",
        "            for j in range(s+1,e):\n",
        "                coords[j] = coords[j-1] + [5.95,0,0]\n",
        "        preds_adapted = [coords]  # one candidate\n",
        "\n",
        "    for i_pred, adapted_coords in enumerate(preds_adapted):\n",
        "        # apply residuals prediction for all residues in this adapted_coords\n",
        "        L = len(seq)\n",
        "        feat_rows = []\n",
        "        W = metadata.get('window', 2)\n",
        "        for i in range(L):\n",
        "            f = {}\n",
        "            f['pos'] = i\n",
        "            f['pos_norm'] = i / max(1, L-1)\n",
        "            f['seq_len'] = L\n",
        "            f['sim_score'] = 0.0\n",
        "            f['ax'], f['ay'], f['az'] = adapted_coords[i]\n",
        "            w = seq_window_onehot(seq, i, W=W)\n",
        "            for j, val in enumerate(w):\n",
        "                f[f'w_{j}'] = float(val)\n",
        "            feat_rows.append(f)\n",
        "        feat_df = pd.DataFrame(feat_rows)\n",
        "        # ensure column order\n",
        "        for c in feature_cols:\n",
        "            if c not in feat_df.columns:\n",
        "                feat_df[c] = 0.0\n",
        "        feat_df = feat_df[feature_cols]\n",
        "\n",
        "        resid_pred = predict_residuals_for_adapted(feat_df)  # (L,3)\n",
        "        corrected = adapted_coords + resid_pred\n",
        "        refined = adaptive_rna_constraints(corrected, tid, confidence=0.8, passes=2)\n",
        "\n",
        "        # fill map by ID\n",
        "        for j in range(L):\n",
        "            total_rows += 1\n",
        "            ID = f\"{tid}_{j+1}\"\n",
        "            # create base entry if not exists\n",
        "            if ID not in pred_map:\n",
        "                # initialize all columns x_1..x_5,y_1..y_5,z_1..z_5 with NaN (will fill later)\n",
        "                d = {'ID': ID, 'resname': seq[j], 'resid': j+1}\n",
        "                for k in range(1,6):\n",
        "                    d[f'x_{k}'] = np.nan\n",
        "                    d[f'y_{k}'] = np.nan\n",
        "                    d[f'z_{k}'] = np.nan\n",
        "                pred_map[ID] = d\n",
        "            # set corresponding triplet (i_pred may be >=5 if function generates more; limit to 5)\n",
        "            col_idx = i_pred + 1\n",
        "            if col_idx <= 5:\n",
        "                pred_map[ID][f'x_{col_idx}'] = float(refined[j,0])\n",
        "                pred_map[ID][f'y_{col_idx}'] = float(refined[j,1])\n",
        "                pred_map[ID][f'z_{col_idx}'] = float(refined[j,2])\n",
        "\n",
        "# Convert to DataFrame and sort by ID/resid (optional: keep test_seqs order)\n",
        "rows = list(pred_map.values())\n",
        "sub = pd.DataFrame(rows)\n",
        "# ensure expected columns and order: ID,resname,resid,x_1,y_1,z_1,...,x_5,y_5,z_5\n",
        "cols = ['ID','resname','resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\n",
        "for c in cols:\n",
        "    if c not in sub.columns:\n",
        "        sub[c] = 0.0\n",
        "sub = sub[cols]\n",
        "\n",
        "# clip coords and save\n",
        "coord_cols = [c for c in cols if c.startswith(('x_','y_','z_'))]\n",
        "sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"submission.csv saved with\", len(sub), \"rows.\")\n",
        "sub.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-16T21:19:33.411108Z",
          "iopub.execute_input": "2026-02-16T21:19:33.411412Z",
          "iopub.status.idle": "2026-02-16T21:22:43.574419Z",
          "shell.execute_reply.started": "2026-02-16T21:19:33.411386Z",
          "shell.execute_reply": "2026-02-16T21:22:43.57355Z"
        },
        "id": "KgwIXS_1yMLF"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}